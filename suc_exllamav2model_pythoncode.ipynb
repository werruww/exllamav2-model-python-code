{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TGEPQyCv0GgY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MP3sqKGu4h-I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b1f9c4b-7fc3-4f97-e66f-97afd76da580"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'exllamav2'...\n",
            "remote: Enumerating objects: 8168, done.\u001b[K\n",
            "remote: Counting objects: 100% (3421/3421), done.\u001b[K\n",
            "remote: Compressing objects: 100% (983/983), done.\u001b[K\n",
            "remote: Total 8168 (delta 2515), reused 2478 (delta 2438), pack-reused 4747 (from 2)\u001b[K\n",
            "Receiving objects: 100% (8168/8168), 21.95 MiB | 14.59 MiB/s, done.\n",
            "Resolving deltas: 100% (5887/5887), done.\n",
            "/content/exllamav2\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 1)) (2.2.2)\n",
            "Collecting ninja (from -r requirements.txt (line 2))\n",
            "  Downloading ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 3)) (0.45.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 4)) (75.2.0)\n",
            "Collecting fastparquet (from -r requirements.txt (line 5))\n",
            "  Downloading fastparquet-2024.11.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: torch>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 6)) (2.6.0+cu124)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 7)) (0.5.3)\n",
            "Requirement already satisfied: sentencepiece>=0.1.97 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 8)) (0.2.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 9)) (2.18.0)\n",
            "Requirement already satisfied: websockets in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 10)) (15.0.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 11)) (2024.11.6)\n",
            "Collecting numpy~=1.26.4 (from -r requirements.txt (line 12))\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m860.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tokenizers in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 13)) (0.21.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 14)) (13.9.4)\n",
            "Requirement already satisfied: pillow>=9.1.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 15)) (11.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->-r requirements.txt (line 1)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->-r requirements.txt (line 1)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->-r requirements.txt (line 1)) (2025.2)\n",
            "Requirement already satisfied: cramjam>=2.3 in /usr/local/lib/python3.11/dist-packages (from fastparquet->-r requirements.txt (line 5)) (2.9.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from fastparquet->-r requirements.txt (line 5)) (2025.3.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from fastparquet->-r requirements.txt (line 5)) (24.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.2.0->-r requirements.txt (line 6)) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2.0->-r requirements.txt (line 6)) (4.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.2.0->-r requirements.txt (line 6)) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2.0->-r requirements.txt (line 6)) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.2.0->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.2.0->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.2.0->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.2.0->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.2.0->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.2.0->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.2.0->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.2.0->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.2.0->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2.0->-r requirements.txt (line 6)) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2.0->-r requirements.txt (line 6)) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2.0->-r requirements.txt (line 6)) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.2.0->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2.0->-r requirements.txt (line 6)) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2.0->-r requirements.txt (line 6)) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.2.0->-r requirements.txt (line 6)) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers->-r requirements.txt (line 13)) (0.30.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->-r requirements.txt (line 14)) (3.0.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->-r requirements.txt (line 13)) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->-r requirements.txt (line 13)) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->-r requirements.txt (line 13)) (4.67.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->-r requirements.txt (line 14)) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 1)) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.2.0->-r requirements.txt (line 6)) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers->-r requirements.txt (line 13)) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers->-r requirements.txt (line 13)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers->-r requirements.txt (line 13)) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers->-r requirements.txt (line 13)) (2025.1.31)\n",
            "Downloading ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m422.8/422.8 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastparquet-2024.11.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m76.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m55.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m44.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m78.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m100.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, ninja, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, fastparquet\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed fastparquet-2024.11.0 ninja-1.11.1.4 numpy-1.26.4 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/turboderp/exllamav2\n",
        "%cd exllamav2\n",
        "!pip install -r requirements.txt\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/exllamav2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ibAYqFpEZA2",
        "outputId": "99cb15cf-4f71-441b-99ae-97aa609cddb4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/exllamav2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git lfs install"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rfs0P6Hw8Vw5",
        "outputId": "3e476b97-239b-42d8-b326-faf0dad544b2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated git hooks.\n",
            "Git LFS initialized.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone -b  2.5bpw https://huggingface.co/turboderp/Mistral-7B-instruct-exl2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "atK22SRm-gUX",
        "outputId": "5e4035b2-6a32-40af-a7f4-f7b260e09a3e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Mistral-7B-instruct-exl2'...\n",
            "remote: Enumerating objects: 79, done.\u001b[K\n",
            "remote: Total 79 (delta 0), reused 0 (delta 0), pack-reused 79 (from 1)\u001b[K\n",
            "Unpacking objects: 100% (79/79), 646.78 KiB | 3.57 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python test_inference.py -m  Mistral-7B-instruct-exl2 -p \"Once upon a time,\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pdbfL8x95vdc",
        "outputId": "cc93a965-e70c-4e60-b2e2-e0e3f13022f3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading exllamav2_ext extension (JIT)...\n",
            "\u001b[2KBuilding C++/CUDA extension \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[33m0:13:20\u001b[0m \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h -- Model: Mistral-7B-instruct-exl2\n",
            " -- Options: []\n",
            "\u001b[2KLoading: Mistral-7B-instruct-exl2 \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[33m0:00:02\u001b[0m \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h -- Loaded model in 2.8131 seconds\n",
            " -- Loading tokenizer...\n",
            " -- Warmup...\n",
            " -- Generating...\n",
            "\n",
            "Once upon a time, in a land far, far away, there lived a beautiful princess named Isabella. She lived in a magnificent castle with her father, the king, and her mother, the queen. Despite her beauty, Isabella was not happy. She was unhappy because she was lonely, and she longed for someone to share her life with.\n",
            "\n",
            "One day, Isabella's father decided that it was time for her to marry. He summoned all the eligible princes from the surrounding kingdoms to come to the castle and meet Isabella. The princes arrived, each one hoping to win Isabella\n",
            "\n",
            " -- Response generated in 2.55 seconds, 128 tokens, 50.19 tokens/second (includes prompt eval.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python test_inference.py -m  Mistral-7B-instruct-exl2 -p \"Write 10 points about places to visit in Europe\" --tokens 1024\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8oMxuQtTA7-i",
        "outputId": "7fc23508-0aad-4f05-84a0-70af75a84b79"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " -- Model: Mistral-7B-instruct-exl2\n",
            " -- Options: []\n",
            "\u001b[2KLoading: Mistral-7B-instruct-exl2 \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[33m0:00:02\u001b[0m \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h -- Loaded model in 2.7970 seconds\n",
            " -- Loading tokenizer...\n",
            " -- Warmup...\n",
            " -- Generating...\n",
            "\n",
            "Write 10 points about places to visit in Europe\n",
            "\n",
            "1. The Eiffel Tower in Paris, France is a must-see landmark that offers breathtaking views of the city.\n",
            "2. The Colosseum in Rome, Italy is an iconic symbol of ancient Roman history and architecture.\n",
            "3. The Louvre Museum in Paris, France is one of the world's most famous art museums, featuring works by Leonardo da Vinci, Michelangelo, and other great artists.\n",
            "4. The Neuschwansteinnasium in Germany is a stunning castle nestled in the Bavarian Alps, with breathtaking views of the surrounding countryside.\n",
            "5. The Alhambra in Granada, Spain is a magnificent palace-fortress complex that showcases the exquisite artistry of Moorish architecture.\n",
            "6. The Palace of Knights' Templar in Paris, France is a historic monument dedicated to the medieval Knights Templar, a Christian military order.\n",
            "7. The Sagrada Familia in Barcelona, Spain is a stunning basilica designed by Antoni Gaudí, featuring intricate details and innovative design elements.\n",
            "8. The Brandenburg Gate in Berlin, Germany is a symbol of the city's history and a powerful reminder of its past.\n",
            "9. The Palace of Versailles in France is a grand chateau and gardens that showcase the opulence of the French monarchy.\n",
            "10. The Tower of London in England is a historic fortress that has played a key role in British history for centuries, with the Crown Jewels housed within its walls.\n",
            "\n",
            "These are just a few of the many amazing places to visit in Europe. Whether you're interested in history, art, architecture, or nature, there's something for everyone to enjoy in this fascinating continent. So pack your bags, set your sights, and get ready to explore the wonders of Europe! \n",
            "\n",
            "#places #travel #tourism #destination #Europe #travelguide #adventure #culture #heritage #sightseeing #explore #discover #tourismguide #see #visit #exploreeurope #europeanjourney #europeantour #europeanadventure #tourisminEurope #travelgoals #travelideas #tourismus #europeancountries #europeanregions #europeandestinations #europeanhotspots #europeanadventures #europeanculture #europeanlandmarks #europeancities #europeanheritage #europeanmonuments #europeandownloads #europeandreams #europeanjourneys #europeantours #europeandventures #europeanvoyages #europeandwonders #europeanexploration #europeandiscovery #europeandescubment #europeandreams #europeandiscoveries #europeandventures #europeandvoyages #europeandwonders #europeanddiscovery #europeandescoveries #europeandjourneys #europeandvoyages #europeandexploration #europeanddiscovery #europeandexploration #europeandiscovery #europeandiscveries #europeandvoyages #europeandexploration #europeandiscovery #europeandexploration #europeanddreams #europeandiscovery #europeandvoyages #europeandexploration #europeandexploration #europeanddiscovery #europeandiscovery #europeanddiscovery #europeandvoyages #europeandexploration #europeanddiscovery #europeanddiscovery #europeanddiscovery #europeanddiscovery #europeanddiscovery #europeanddiscovery #europeanddiscovery #europeanddiscovery #europeanddiscovery #europeanddiscovery #europeanddiscovery #europeanddiscovery #europeanddiscovery #europeanddiscovery #europeanddiscovery #europeanddiscovery #europeanddiscovery #europeanddiscovery #europeanddiscovery #europeanddiscovery #europeanddiscovery #europeanddiscovery #europeanddiscovery #europeanddiscovery #europeanddiscovery #europeanddiscovery #europeanddiscovery #europeanddiscovery #europeanddiscovery #europeanddiscovery #europeanddiscovery #europeanddiscovery #europeanddiscovery #europeanddiscovery #europeanddiscovery #europeanddiscovery #europeanddiscovery #europeand\n",
            "\n",
            " -- Response generated in 17.73 seconds, 1024 tokens, 57.76 tokens/second (includes prompt eval.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import sys, os\n",
        "sys.path.append('/content/exllamav2')\n",
        "\n",
        "from exllamav2 import(\n",
        "    ExLlamaV2,\n",
        "    ExLlamaV2Config,\n",
        "    ExLlamaV2Cache,\n",
        "    ExLlamaV2Tokenizer,\n",
        ")\n",
        "\n",
        "from exllamav2.generator import (\n",
        "    ExLlamaV2BaseGenerator,\n",
        "    ExLlamaV2Sampler\n",
        ")\n",
        "\n",
        "import time\n",
        "\n",
        "# Input prompts\n",
        "\n",
        "batch_size = 5\n",
        "\n",
        "prompts = \\\n",
        "[\n",
        "    \"How do I open a can of beans?\",\n",
        "    \"How do I open a can of soup?\",\n",
        "    \"How do I open a can of strawberry jam?\",\n",
        "    \"How do I open a can of raspberry jam?\",\n",
        "    \"What's the tallest building in Paris?\",\n",
        "    \"What's the most populous nation on Earth?\",\n",
        "    \"What's the most populous nation on Mars?\",\n",
        "    \"What do the Mole People actually want and how can we best appease them?\",\n",
        "    \"Why is the sky blue?\",\n",
        "    \"Where is Waldo?\",\n",
        "    \"Who is Waldo?\",\n",
        "    \"Why is Waldo?\",\n",
        "    \"Is it legal to base jump off the Eiffel Tower?\",\n",
        "    \"Is it legal to base jump into a volcano?\",\n",
        "    \"Why are cats better than dogs?\",\n",
        "    \"Why is the Hulk so angry all the time?\",\n",
        "    \"How do I build a time machine?\",\n",
        "    \"Is it legal to grow your own catnip?\"\n",
        "]\n",
        "\n",
        "# Sort by length to minimize padding\n",
        "\n",
        "s_prompts = sorted(prompts, key = len)\n",
        "\n",
        "# Apply prompt format\n",
        "\n",
        "def format_prompt(sp, p):\n",
        "    return f\"[INST] <<SYS>>\\n{sp}\\n<</SYS>>\\n\\n{p} [/INST]\"\n",
        "\n",
        "system_prompt = \"Answer the question to the best of your ability.\"\n",
        "f_prompts = [format_prompt(system_prompt, p) for p in s_prompts]\n",
        "\n",
        "# Split into batches\n",
        "\n",
        "batches = [f_prompts[i:i + batch_size] for i in range(0, len(prompts), batch_size)]\n",
        "\n",
        "# Initialize model and cache\n",
        "\n",
        "model_directory =  \"Mistral-7B-instruct-exl2\"\n",
        "\n",
        "config = ExLlamaV2Config()\n",
        "config.model_dir = model_directory\n",
        "config.prepare()\n",
        "\n",
        "config.max_batch_size = batch_size  # Model instance needs to allocate temp buffers to fit the max batch size\n",
        "\n",
        "model = ExLlamaV2(config)\n",
        "print(\"Loading model: \" + model_directory)\n",
        "\n",
        "cache = ExLlamaV2Cache(model, lazy = True, batch_size = batch_size)  # Cache needs to accommodate the batch size\n",
        "model.load_autosplit(cache)\n",
        "\n",
        "tokenizer = ExLlamaV2Tokenizer(config)\n",
        "\n",
        "# Initialize generator\n",
        "\n",
        "generator = ExLlamaV2BaseGenerator(model, cache, tokenizer)\n",
        "\n",
        "# Sampling settings\n",
        "\n",
        "settings = ExLlamaV2Sampler.Settings()\n",
        "settings.temperature = 0.85\n",
        "settings.top_k = 50\n",
        "settings.top_p = 0.8\n",
        "settings.token_repetition_penalty = 1.05\n",
        "\n",
        "max_new_tokens = 512\n",
        "\n",
        "# generator.warmup()  # Only needed to fully initialize CUDA, for correct benchmarking\n",
        "\n",
        "# Generate for each batch\n",
        "\n",
        "collected_outputs = []\n",
        "for b, batch in enumerate(batches):\n",
        "\n",
        "    print(f\"Batch {b + 1} of {len(batches)}...\")\n",
        "\n",
        "    outputs = generator.generate_simple(batch, settings, max_new_tokens, seed = 1234)\n",
        "\n",
        "    trimmed_outputs = [o[len(p):] for p, o in zip(batch, outputs)]\n",
        "    collected_outputs += trimmed_outputs\n",
        "\n",
        "# Print the results\n",
        "\n",
        "for q, a in zip(s_prompts, collected_outputs):\n",
        "    print(\"---------------------------------------\")\n",
        "    print(\"Q: \" + q)\n",
        "    print(\"A: \" + a)\n",
        "\n",
        "# print(f\"Response generated in {time_total:.2f} seconds, {max_new_tokens} tokens, {max_new_tokens / time_total:.2f} tokens/second\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "d_T8NHdhBJfT",
        "outputId": "f7682d2c-0281-4a29-9708-24c6fe962683"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-4d5c123324c3>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/exllamav2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m from exllamav2 import(\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mExLlamaV2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mExLlamaV2Config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/exllamav2/exllamav2/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexllamav2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mexllamav2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mExLlamaV2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexllamav2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mExLlamaV2CacheBase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexllamav2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mExLlamaV2Cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/exllamav2/exllamav2/model.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexllamav2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmsnorm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mExLlamaV2RMSNorm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexllamav2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayernorm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mExLlamaV2LayerNorm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mexllamav2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mExLlamaV2Attention\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_flash_attn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_xformers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexllamav2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlora\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mExLlamaV2Lora\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexllamav2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mExLlamaV2MLP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/exllamav2/exllamav2/attn.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m'EXLLAMA_NO_SDPA'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcausal_lower_right\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0mhas_lower_right_sdpa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/attention/bias.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallow_in_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_flash_attention_available\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallow_in_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcan_use_flash_attention\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallow_in_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcan_use_efficient_attention\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m   2677\u001b[0m         \u001b[0;31m# Lazy modules\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2678\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_lazy_modules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2679\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\".{name}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2681\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"module '{__name__}' has no attribute '{name}'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_dynamo/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconvert_frame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_frame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresume_execution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregistry\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlist_backends\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlookup_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregister_backend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcallback\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcallback_handler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon_compile_end\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon_compile_start\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_dynamo/convert_frame.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mguards\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGlobalStateGuard\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributed\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_compile_pg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msymbolic_convert\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTensorifyState\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_guards\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompile_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCompileContext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCompileId\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_logging\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstructured\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_logging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTensorifyScalarRestartAnalysis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_guards\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtracing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTracingContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_dynamo/exc.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcounters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_dynamo/utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfft\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         )\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__dir__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mpublic_symbols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'testing'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         public_symbols -= {\n\u001b[1;32m    339\u001b[0m             \u001b[0;34m\"core\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"matrixlib\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/random/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;31m# add these for module-freeze analysis (like PyInstaller)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_common\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_bounded_integers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/random/_pickle.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmtrand\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomState\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_philox\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPhilox\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_pcg64\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPCG64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPCG64DXSM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_sfc64\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSFC64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mnumpy/random/mtrand.pyx\u001b[0m in \u001b[0;36minit numpy.random.mtrand\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade numpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hS3kzT2W5Xn3",
        "outputId": "7f72b946-ce31-47d1-9449-9952077a1732"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Collecting numpy\n",
            "  Downloading numpy-2.2.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.2.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m88.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.4 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-2.2.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/exllamav2\n",
        "import sys, os\n",
        "sys.path.append('/content/exllamav2')\n",
        "\n",
        "from exllamav2 import(\n",
        "    ExLlamaV2,\n",
        "    ExLlamaV2Config,\n",
        "    ExLlamaV2Cache,\n",
        "    ExLlamaV2Tokenizer,\n",
        ")\n",
        "\n",
        "from exllamav2.generator import (\n",
        "    ExLlamaV2BaseGenerator,\n",
        "    ExLlamaV2Sampler\n",
        ")\n",
        "\n",
        "import time\n",
        "\n",
        "# Input prompts\n",
        "\n",
        "batch_size = 5\n",
        "\n",
        "prompts = \\\n",
        "[\n",
        "    \"How do I open a can of beans?\",\n",
        "    \"How do I open a can of soup?\",\n",
        "    \"How do I open a can of strawberry jam?\",\n",
        "    \"How do I open a can of raspberry jam?\",\n",
        "    \"What's the tallest building in Paris?\",\n",
        "    \"What's the most populous nation on Earth?\",\n",
        "    \"What's the most populous nation on Mars?\",\n",
        "    \"What do the Mole People actually want and how can we best appease them?\",\n",
        "    \"Why is the sky blue?\",\n",
        "    \"Where is Waldo?\",\n",
        "    \"Who is Waldo?\",\n",
        "    \"Why is Waldo?\",\n",
        "    \"Is it legal to base jump off the Eiffel Tower?\",\n",
        "    \"Is it legal to base jump into a volcano?\",\n",
        "    \"Why are cats better than dogs?\",\n",
        "    \"Why is the Hulk so angry all the time?\",\n",
        "    \"How do I build a time machine?\",\n",
        "    \"Is it legal to grow your own catnip?\"\n",
        "]\n",
        "\n",
        "# Sort by length to minimize padding\n",
        "\n",
        "s_prompts = sorted(prompts, key = len)\n",
        "\n",
        "# Apply prompt format\n",
        "\n",
        "def format_prompt(sp, p):\n",
        "    return f\"[INST] <<SYS>>\\n{sp}\\n<</SYS>>\\n\\n{p} [/INST]\"\n",
        "\n",
        "system_prompt = \"Answer the question to the best of your ability.\"\n",
        "f_prompts = [format_prompt(system_prompt, p) for p in s_prompts]\n",
        "\n",
        "# Split into batches\n",
        "\n",
        "batches = [f_prompts[i:i + batch_size] for i in range(0, len(prompts), batch_size)]\n",
        "\n",
        "# Initialize model and cache\n",
        "\n",
        "model_directory =  \"Mistral-7B-instruct-exl2\"\n",
        "\n",
        "config = ExLlamaV2Config()\n",
        "config.model_dir = model_directory\n",
        "config.prepare()\n",
        "\n",
        "config.max_batch_size = batch_size  # Model instance needs to allocate temp buffers to fit the max batch size\n",
        "\n",
        "model = ExLlamaV2(config)\n",
        "print(\"Loading model: \" + model_directory)\n",
        "\n",
        "cache = ExLlamaV2Cache(model, lazy = True, batch_size = batch_size)  # Cache needs to accommodate the batch size\n",
        "model.load_autosplit(cache)\n",
        "\n",
        "tokenizer = ExLlamaV2Tokenizer(config)\n",
        "\n",
        "# Initialize generator\n",
        "\n",
        "generator = ExLlamaV2BaseGenerator(model, cache, tokenizer)\n",
        "\n",
        "# Sampling settings\n",
        "\n",
        "settings = ExLlamaV2Sampler.Settings()\n",
        "settings.temperature = 0.85\n",
        "settings.top_k = 50\n",
        "settings.top_p = 0.8\n",
        "settings.token_repetition_penalty = 1.05\n",
        "\n",
        "max_new_tokens = 512\n",
        "\n",
        "# generator.warmup()  # Only needed to fully initialize CUDA, for correct benchmarking\n",
        "\n",
        "# Generate for each batch\n",
        "\n",
        "collected_outputs = []\n",
        "for b, batch in enumerate(batches):\n",
        "\n",
        "    print(f\"Batch {b + 1} of {len(batches)}...\")\n",
        "\n",
        "    outputs = generator.generate_simple(batch, settings, max_new_tokens, seed = 1234)\n",
        "\n",
        "    trimmed_outputs = [o[len(p):] for p, o in zip(batch, outputs)]\n",
        "    collected_outputs += trimmed_outputs\n",
        "\n",
        "# Print the results\n",
        "\n",
        "for q, a in zip(s_prompts, collected_outputs):\n",
        "    print(\"---------------------------------------\")\n",
        "    print(\"Q: \" + q)\n",
        "    print(\"A: \" + a)\n",
        "\n",
        "# print(f\"Response generated in {time_total:.2f} seconds, {max_new_tokens} tokens, {max_new_tokens / time_total:.2f} tokens/second\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HM9ZKNsC5am-",
        "outputId": "48ce97bd-ebd0-4177-8d47-8c9e107fb820"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/exllamav2\n",
            "Loading model: Mistral-7B-instruct-exl2\n",
            "Batch 1 of 4...\n",
            "Batch 2 of 4...\n",
            "Batch 3 of 4...\n",
            "Batch 4 of 4...\n",
            "---------------------------------------\n",
            "Q: Who is Waldo?\n",
            "A:  Waldo is a fictional character in the Harry Potter series by J.K. Rowling. He is a member of the Order of the Phoenix and is known for his bravery and loyalty to his friends. He is also the recipient of the Gryffindor Firefly Award in the third year of the series.\n",
            "---------------------------------------\n",
            "Q: Why is Waldo?\n",
            "A:  Waldo is a fictional character in the Harry Potter series created by J.K. Rowling. He is a member of the Hogwarts house of Gryffindor and plays a significant role in the first book of the series, \"Harry Potter and the Philosopher's Stone.\" Waldo is portrayed as a brave and resourceful character who is willing to risk his life to protect others. Throughout the series, Waldo is known for his bravery and selflessness, and he is often seen as a heroic figure.\n",
            "---------------------------------------\n",
            "Q: Where is Waldo?\n",
            "A:  Waldo is a fictional character from the Harry Potter series by J.K. Rowling. He is a member of the Order of the Phoenix and is known for his eccentricities and unusual behavior. He is not a real person or place, so there is no known location where he resides.\n",
            "---------------------------------------\n",
            "Q: Why is the sky blue?\n",
            "A:  The sky appears blue because of a phenomenon called Rayleigh scattering. When sunlight enters Earth's atmosphere, it collides with particles in the air such as molecules and tiny dust particles. The light is then scattered in many directions by these collisions. Blue light has a shorter wavelength than other colors and is more easily scattered, so it is more visible in the sky. This effect is more pronounced during the day when the sun is high in the sky. At sunrise and sunset, the light has to pass through more of the Earth's atmosphere, which causes some of the blue light to be absorbed, making the sky appear more reddish.\n",
            "---------------------------------------\n",
            "Q: How do I open a can of soup?\n",
            "A:  To open a can of soup, follow these steps:\n",
            "\n",
            "1. Hold the can upright and locate the tab that is usually at the top of the can.\n",
            "2. Use your fingers or a can opener to push down on the tab to release the lid.\n",
            "3. Lift the lid off the can and pour the soup into a bowl or onto a plate.\n",
            "4. Enjoy your soup!\n",
            "---------------------------------------\n",
            "Q: How do I open a can of beans?\n",
            "A:  To open a can of beans, follow these steps:\n",
            "\n",
            "1. Hold the can upright and press down on the tab or lid to release it from the can.\n",
            "2. Open the lid by pulling it towards you or using an opener.\n",
            "3. Once the lid is off, use a can opener to cut the can open, making sure to stay away from the sharp blades.\n",
            "4. Drain the beans into a bowl or onto a plate.\n",
            "5. Rinse the can in water to remove any residue.\n",
            "6. Dispose of the can properly.\n",
            "\n",
            "It's important to remember not to pour the beans directly into the can, as this could cause the sharp metal edges to cut into the beans.\n",
            "---------------------------------------\n",
            "Q: Why are cats better than dogs?\n",
            "A:  Cats and dogs are both beloved pets, but they have different personalities and characteristics that people find appealing. Some people prefer cats because they are independent and low maintenance, while others prefer dogs because they are loyal and social companions. Ultimately, it comes down to individual preference and what fits best with a specific household or lifestyle.\n",
            "---------------------------------------\n",
            "Q: How do I build a time machine?\n",
            "A:  Building a time machine requires a great deal of knowledge and expertise in physics, engineering, and technology. Here are some general steps to consider:\n",
            "\n",
            "1. Determine the type of time machine you want to build. Some popular types include a personal time machine, a group time machine, or a time capsule.\n",
            "2. Research the science behind time travel, including the theory of relativity and its implications on time and space.\n",
            "3. Design and construct a device that can manipulate time and space. This could involve developing a device that creates a wormhole or using energy fields to create a time portal.\n",
            "4. Test and refine your design until it is functioning properly.\n",
            "5. Consider the ethical implications of time travel and ensure that your design is safe and responsible.\n",
            "6. Build a prototype and test it in a controlled environment.\n",
            "7. Continue to refine and improve your design until it is ready for full-scale testing.\n",
            "8. Document your process and findings, and share your work with others in the scientific community.\n",
            "\n",
            "It's important to note that building a time machine is a complex task that requires a significant amount of resources and expertise. It has not yet been proven scientifically possible to build a time machine, so it remains a topic of ongoing research and debate.\n",
            "---------------------------------------\n",
            "Q: Is it legal to grow your own catnip?\n",
            "A:  It is legal to grow your own catnip in some countries, but it may be subject to certain regulations or restrictions depending on the specific laws and regulations of that country. For example, in the United States, the legal status of catnip can vary from state to state, and some states have specific laws regarding the legality of growing catnip. In other countries, such as Canada, there are no specific laws regarding the legality of growing catnip, but it may be subject to customs regulations if it is imported into the country. It is always best to check with the relevant authorities to confirm the legality of growing catnip in a particular location.\n",
            "---------------------------------------\n",
            "Q: How do I open a can of raspberry jam?\n",
            "A:  To open a can of raspberry jam, follow these steps:\n",
            "\n",
            "1. Hold the can upright and ensure that it is firmly closed.\n",
            "2. Locate the tab or ring to release the lid by pulling on it with your thumb or finger.\n",
            "3. Lift the tab or ring upwards and over the edge of the can.\n",
            "4. Pull the tab or ring off the can, releasing any pressure that is holding it in place.\n",
            "5. If desired, use the top of the can as a makeshift handle to pour out the jam. Alternatively, you can carefully lift the entire can by its bottom.\n",
            "---------------------------------------\n",
            "Q: What's the tallest building in Paris?\n",
            "A:  The tallest building in Paris is the Eiffel Tower, which stands at 324 meters (1,063 feet) tall.\n",
            "---------------------------------------\n",
            "Q: How do I open a can of strawberry jam?\n",
            "A:  To open a can of strawberry jam, follow these steps:\n",
            "\n",
            "1. Hold the can upright and look for the tab or pull tab.\n",
            "2. Grasp the tab with your hand and pull it towards or upwards.\n",
            "3. Once the tab is pulled out, you can now see the contents of the can.\n",
            "4. Take off the lid from the jar by pulling it upwards or backward.\n",
            "5. After taking off the lid, you can use a spoon or a container to scoop out the jam.\n",
            "6. If the jam is in a plastic container, be careful not to spill any of it.\n",
            "7. Enjoy your strawberry jam!\n",
            "---------------------------------------\n",
            "Q: Why is the Hulk so angry all the time?\n",
            "A:  The Hulk's anger is caused by a combination of factors, including his heightened senses and the constant stress of feeling out of place in society. He is also triggered by certain stimuli, such as loud noises or physical contact that may not be intended as aggression. Additionally, the Hulk has a complex personality and is often misunderstood due to his extreme temperament and lack of social skills.\n",
            "---------------------------------------\n",
            "Q: What's the most populous nation on Mars?\n",
            "A:  As of now, there is no permanent population on Mars. However, NASA has sent several robotic missions to Mars to explore and learn about the planet.\n",
            "---------------------------------------\n",
            "Q: Is it legal to base jump into a volcano?\n",
            "A:  Base jumping into a volcano is not legal, as it is highly dangerous and potentially life-threatening. It is illegal in most countries due to safety concerns and risk of injury or death.\n",
            "---------------------------------------\n",
            "Q: What's the most populous nation on Earth?\n",
            "A:  The most populous nation on Earth is China, with over 1.4 billion people as of 2021.\n",
            "---------------------------------------\n",
            "Q: Is it legal to base jump off the Eiffel Tower?\n",
            "A:  No, it is not legal to base jump off the Eiffel Tower. The tower itself has safety measures in place to prevent such activities, and it is also illegal to engage in such activities without proper authorization and adherence to safety regulations.\n",
            "---------------------------------------\n",
            "Q: What do the Mole People actually want and how can we best appease them?\n",
            "A:  It is difficult to say exactly what the Mole People want without more context or information about their goals and desires. However, it is possible that they may want to improve their living conditions, have access to better resources and opportunities for growth, and perhaps to gain more independence and autonomy. To appease them, it may be helpful to provide them with resources and support that can help them achieve these goals, such as education and training programs, access to healthcare and basic needs, and opportunities for social interaction and community building. It is also important to listen to their concerns and needs and ensure that their rights and freedoms are respected.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from exllamav2 import (\n",
        "    ExLlamaV2,\n",
        "    ExLlamaV2Config,\n",
        "    ExLlamaV2Cache,\n",
        "    ExLlamaV2Tokenizer,\n",
        ")\n",
        "\n",
        "from exllamav2.generator import (\n",
        "    ExLlamaV2StreamingGenerator,\n",
        "    ExLlamaV2Sampler\n",
        ")\n",
        "\n",
        "import time\n",
        "\n",
        "\n",
        "tokenizer = ExLlamaV2Tokenizer(config)\n",
        "\n",
        "# Initialize generator\n",
        "\n",
        "generator = ExLlamaV2StreamingGenerator(model, cache, tokenizer)\n",
        "\n",
        "# Settings\n",
        "\n",
        "settings = ExLlamaV2Sampler.Settings()\n",
        "settings.temperature = 0.85\n",
        "settings.top_k = 50\n",
        "settings.top_p = 0.8\n",
        "settings.top_a = 0.0\n",
        "settings.token_repetition_penalty = 1.05\n",
        "settings.disallow_tokens(tokenizer, [tokenizer.eos_token_id])\n",
        "\n",
        "max_new_tokens = 512\n",
        "\n",
        "# Prompt\n",
        "\n",
        "prompt = \"Our story begins in the Scottish town of Auchtermuchty, where once\"\n",
        "\n",
        "input_ids = tokenizer.encode(prompt)\n",
        "prompt_tokens = input_ids.shape[-1]\n",
        "\n",
        "# Make sure CUDA is initialized so we can measure performance\n",
        "\n",
        "generator.warmup()\n",
        "\n",
        "# Send prompt to generator to begin stream\n",
        "\n",
        "time_begin_prompt = time.time()\n",
        "\n",
        "print (prompt, end = \"\")\n",
        "sys.stdout.flush()\n",
        "\n",
        "generator.set_stop_conditions([])\n",
        "generator.begin_stream(input_ids, settings)\n",
        "\n",
        "# Streaming loop. Note that repeated calls to sys.stdout.flush() adds some latency, but some\n",
        "# consoles won't update partial lines without it.\n",
        "\n",
        "time_begin_stream = time.time()\n",
        "generated_tokens = 0\n",
        "\n",
        "while True:\n",
        "    chunk, eos, _ = generator.stream()\n",
        "    generated_tokens += 1\n",
        "    print (chunk, end = \"\")\n",
        "    sys.stdout.flush()\n",
        "    if eos or generated_tokens == max_new_tokens: break\n",
        "\n",
        "time_end = time.time()\n",
        "\n",
        "time_prompt = time_begin_stream - time_begin_prompt\n",
        "time_tokens = time_end - time_begin_stream\n",
        "\n",
        "print()\n",
        "print()\n",
        "print(f\"Prompt processed in {time_prompt:.2f} seconds, {prompt_tokens} tokens, {prompt_tokens / time_prompt:.2f} tokens/second\")\n",
        "print(f\"Response generated in {time_tokens:.2f} seconds, {generated_tokens} tokens, {generated_tokens / time_tokens:.2f} tokens/second\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FrY1j9IzDBOn",
        "outputId": "08a47875-92f2-40e1-9d3a-d7b0c1f2283a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Our story begins in the Scottish town of Auchtermuchty, where once a year, at the end of May, a procession of people in white robes, holding torches and playing drums, would make their way through the streets.\n",
            "\n",
            "The people of Auchtermuchty were proud of their traditions, but they had no idea how much danger lay ahead. One day, as the procession was approaching the main square, a young boy named Ewan saw something strange in the sky. He ran to his father, who happened to be part of the procession, and told him that he saw a dark cloud moving towards them. But his father dismissed his worries, telling him that it was probably just a passing storm.\n",
            "\n",
            "As the procession continued, the dark cloud grew larger and more ominous. Suddenly, a bolt of lightning struck the ground, causing the entire procession to stop dead in its tracks. The people looked around in confusion, wondering what had just happened. But then, as if out of nowhere, a group of shadowy figures appeared before them, holding weapons and wearing black robes.\n",
            "\n",
            "The people of Auchtermuchty were terrified, but they had no idea what these figures were or what they wanted. They tried to run away, but the figures chased them down the streets, firing arrows and throwing rocks.\n",
            "\n",
            "In the chaos, Ewan managed to grab a torch and run towards the shadowy figures. He threw the torch at them, hoping to scare them away. But instead, the figures grabbed the torch and used it to light their own torches.\n",
            "\n",
            "Ewan realized too late that these figures were not friendly. They were vampires, and they had come to Auchtermuchty to drain the blood of the townspeople.\n",
            "\n",
            "The people of Auchtermuchty fought bravely, using their torches and drums to defend themselves against the vampires. But they were no match for the vampires' strength and speed. In the end, they were all killed, except for Ewan.\n",
            "\n",
            "After the battle, Ewan wandered through the ruined town, looking for a way to stop the vampires. He found out that the only way to defeat them was to light a torch in the shape of a cross. He lit a torch and placed it on the ground, hoping that it would attract the vampires.\n",
            "\n",
            "The vampires were drawn to the light, and they began to approach the torch. But as they got closer, they were suddenly consumed by flames. The vampires\n",
            "\n",
            "Prompt processed in 0.01 seconds, 15 tokens, 1405.28 tokens/second\n",
            "Response generated in 8.77 seconds, 512 tokens, 58.36 tokens/second\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from exllamav2 import (\n",
        "    ExLlamaV2,\n",
        "    ExLlamaV2Config,\n",
        "    ExLlamaV2Cache,\n",
        "    ExLlamaV2Tokenizer,\n",
        ")\n",
        "\n",
        "from exllamav2.generator import (\n",
        "    ExLlamaV2StreamingGenerator,\n",
        "    ExLlamaV2Sampler\n",
        ")\n",
        "\n",
        "import time\n",
        "\n",
        "\n",
        "tokenizer = ExLlamaV2Tokenizer(config)\n",
        "\n",
        "# Initialize generator\n",
        "\n",
        "generator = ExLlamaV2StreamingGenerator(model, cache, tokenizer)\n",
        "\n",
        "# Settings\n",
        "\n",
        "settings = ExLlamaV2Sampler.Settings()\n",
        "settings.temperature = 0.85\n",
        "settings.top_k = 50\n",
        "settings.top_p = 0.8\n",
        "settings.top_a = 0.0\n",
        "settings.token_repetition_penalty = 1.05\n",
        "settings.disallow_tokens(tokenizer, [tokenizer.eos_token_id])\n",
        "\n",
        "max_new_tokens = 512\n",
        "\n",
        "# Prompt\n",
        "\n",
        "prompt = \"Who is Napoleon Bonaparte?\"\n",
        "\n",
        "input_ids = tokenizer.encode(prompt)\n",
        "prompt_tokens = input_ids.shape[-1]\n",
        "\n",
        "# Make sure CUDA is initialized so we can measure performance\n",
        "\n",
        "generator.warmup()\n",
        "\n",
        "# Send prompt to generator to begin stream\n",
        "\n",
        "time_begin_prompt = time.time()\n",
        "\n",
        "print (prompt, end = \"\")\n",
        "sys.stdout.flush()\n",
        "\n",
        "generator.set_stop_conditions([])\n",
        "generator.begin_stream(input_ids, settings)\n",
        "\n",
        "# Streaming loop. Note that repeated calls to sys.stdout.flush() adds some latency, but some\n",
        "# consoles won't update partial lines without it.\n",
        "\n",
        "time_begin_stream = time.time()\n",
        "generated_tokens = 0\n",
        "\n",
        "while True:\n",
        "    chunk, eos, _ = generator.stream()\n",
        "    generated_tokens += 1\n",
        "    print (chunk, end = \"\")\n",
        "    sys.stdout.flush()\n",
        "    if eos or generated_tokens == max_new_tokens: break\n",
        "\n",
        "time_end = time.time()\n",
        "\n",
        "time_prompt = time_begin_stream - time_begin_prompt\n",
        "time_tokens = time_end - time_begin_stream\n",
        "\n",
        "print()\n",
        "print()\n",
        "print(f\"Prompt processed in {time_prompt:.2f} seconds, {prompt_tokens} tokens, {prompt_tokens / time_prompt:.2f} tokens/second\")\n",
        "print(f\"Response generated in {time_tokens:.2f} seconds, {generated_tokens} tokens, {generated_tokens / time_tokens:.2f} tokens/second\")"
      ],
      "metadata": {
        "id": "qFV973pDEKwi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d5edb6b-0466-4db8-ef01-8f091a49240a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Who is Napoleon Bonaparte?\n",
            "\n",
            "Napoleon Bonaparte was a French military and political leader from 1799 to 1821. He rose from the ranks of a second lieutenant in the French army to become First Consul of France in 1804, and then Emperor of the French in 1804. He was a major figure in European history and played a key role in shaping Europe as we know it today.\n",
            "\n",
            "What did Napoleon Bonaparte do?\n",
            "\n",
            "Napoleon Bonaparte conquered much of Europe. He defeated the Austrians, Prussians, Russians, and Britons in numerous battles, and his conquests expanded the French Empire to include much of Europe. He also introduced many reforms that modernized and liberalized the French legal system and administration.\n",
            "\n",
            "What were Napoleon Bonaparte's accomplishments?\n",
            "\n",
            "Napoleon Bonaparte's accomplishments include:\n",
            "\n",
            "1. Establishing the Napoleonic Code, which served as the basis for the new civil code of France.\n",
            "2. Implementing many reforms in the legal system, including the abolition of feudalism and the establishment of the Napoleonic Code.\n",
            "3. Expanding the French Empire through various military campaigns and conquests.\n",
            "4. Promoting nationalism and encouraging the development of a strong French identity.\n",
            "5. Promoting social equality and liberty through the abolition of feudalism and the establishment of a meritocracy.\n",
            "6. Promoting scientific and technological advancements through education and the establishment of research institutions.\n",
            "7. Promoting religious tolerance and religious freedom through the establishment of the Napoleonic Code.\n",
            "\n",
            "What were Napoleon Bonaparte's failures?\n",
            "\n",
            "Napoleon Bonaparte's failures include:\n",
            "\n",
            "1. The defeat at Waterloo in 1815, which marked the end of his rule as Emperor of France.\n",
            "2. The invasion of Russia in 1812, which marked the beginning of his decline as a military leader.\n",
            "3. The execution of King Louis XVIII of France in 1821, which marked the end of his rule as Emperor of France.\n",
            "4. The defeat at the Battle of Jena in 1805, which marked the beginning of his decline as a military leader.\n",
            "5. The defeat at the Battle of Wagram in 1806\n",
            "\n",
            "Prompt processed in 0.01 seconds, 7 tokens, 586.46 tokens/second\n",
            "Response generated in 8.91 seconds, 512 tokens, 57.46 tokens/second\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NA0ffa9G6LCv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}